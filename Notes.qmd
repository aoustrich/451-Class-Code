---
title: "Notes"
author: "Aaron Oustrich"
format: html
editor: visual
---

# Misc. Quotes

-   "The distribution of the ratios is the ratio of the distributions."
-   "Central Limit Theorem for Bayesians: As sample size increases the posteriors become more normal."
-   "I don't give a rip about unbiased estimators!"
-   "We've got certain laws about probability. Bayesians follow them, Frequentists don't."

# Bayes v. Frequentist

-   Frequentists treat parameters as fixed but unknown. Bayesians treat parameters as random variables.
-   Having unbiased estimators sounds good but they're unbiased to infinity. When we get a single Frequentist estimate we don't know it is unbiased - confidence intervals are in the long run. To account for the violation of the laws of probability Frequentists introduced confidence intervals and null hypothesis testing. Bayesians don't have to do that because they follow the laws of probability.

# Diagnostics

### Gelman-Rubin diagnostic (`coda::gelman.diag`):

-   The ratio of the variance of the posterior distribution to the mean of the posterior distribution. Measures how close the chains are converging to the same point. We want upper confidence intervals to be close to 1. Sometimes called $\hat{R}$ (especially in STAN)

-   There must be at least 2 chains to work

### Raftery-Lewis diagnostic (`coda::raftery.diag`)

-   "Are we comfortable in the tails?": measures how many samples are needed to get a good estimate? Raftery said Dependence factor less than 5 is good - Fellingham likes a Dependence factor less than 3?

-   converting to matrix makes things easier for `rafferty.diag`. `as.matrix` makes an $n \times p$ matrix where $n$ is sum of the length of every chain and $p$ is the number of parameters

### Effective Sample Size (`effectiveSize`:

-   This [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6305549/#:~:text=2.8.-,Effective%20Sample%20Size,-The%20Effective%20Sample) say the ESS "measures the information content, or effectiveness of a sample chain."
    -   Fellingham thinks effective sample size of 5,000 is good.
-   The effective sample size is a function of Autocorrelation. As autocorellation increases, the effective sample size decreases. So if the autocorrelation is messing up the effective sample size, then thinning more might help.

### Autocorrelation (`coda::autocorr.diag`)

-   When the autocorrelation is negative, "some of the diagnostics do funny things" ex: like where the effective size is greater than the number of samples.
-   `autocorr.diag(as.mcmc(allsamps))` where `allsamps` is a matrix of all the samples and `coda::as.mcmc` converts it to a `mcmc` object (don't understand why it's important other than it's a requirement for `autocorr.diag`)

### Watanabe-Akaike Information Criterion ('Widely Available Information Criterion' or WAIC)

-   Want this to be small
-   `lppd` (log-predictive density?): Measures the quality of fit of the model
    -   `Deviance` = -2 \* `lppd`
-   `pWAIC`: Penalty for more complex model

# Prior Distributions

#### Gamma

-   Fellingham prefers shape \> 1 so the curve "flips over" instead of going up to $\infty$ as $x \to 0$

# ANOVA/ANCOVA
